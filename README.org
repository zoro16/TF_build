#+TITLE: Build TensorFlow Docker images for CPU and GPU
#+OPTIONS: H:1 num:nil toc:nil \n:nil @:t ::t |:t ^:{} _:{} *:t TeX:t LaTeX:t


* Setup the docker image
  - Get the repo
    #+BEGIN_SRC BASH
      git clone git@github.com:zoro16/TF_build.git
    #+END_SRC
  - Download Anaconda from [[https://www.anaconda.com/download/#linux][here]]
  - Build docker image
    #+BEGIN_SRC BASH
      docker build -t tf-build-1.8-gpu -f dockerfile/Dockerfile-gpu .
    #+END_SRC
  - Start the container and bind the directory with the source tree
    #+BEGIN_SRC BASH
      nvidia-docker run --rm -it -v $HOME/projects/TF_build:/root/TF_build tf-build-1.8-gpu
    #+END_SRC
  - Configure TensorFlow build
    #+BEGIN_SRC BASH
      cd /root/TF-build/tensorflow/

      ./configure
    #+END_SRC
    #+BEGIN_SRC BASH
      root@dc40c84fcef1:~/TF-build/tensorflow# ./configure
      /root/anaconda3/bin/python
      .
      Extracting Bazel installation...
      You have bazel 0.11.1 installed.
      Please specify the location of python. [Default is /root/anaconda3/bin/python]:


      Found possible Python library paths:
      /root/anaconda3/lib/python3.6/site-packages
      Please input the desired Python library path to use.  Default is [/root/anaconda3/lib/python3.6/site-packages]

      Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y
      jemalloc as malloc support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
      No Google Cloud Platform support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: n
      No Hadoop File System support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
      No Amazon S3 File System support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: n
      No Apache Kafka Platform support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with XLA JIT support? [y/N]: n
      No XLA JIT support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with GDR support? [y/N]: n
      No GDR support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with VERBS support? [y/N]: n
      No VERBS support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
      No OpenCL SYCL support will be enabled for TensorFlow.

      Do you wish to build TensorFlow with CUDA support? [y/N]: y
      CUDA support will be enabled for TensorFlow.

      Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.1


      Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


      Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]:7.1


      Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


      Do you wish to build TensorFlow with TensorRT support? [y/N]: n
      No TensorRT support will be enabled for TensorFlow.

      Please specify a list of comma-separated Cuda compute capabilities you want to build with.
      You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
      Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]5.2,6.0,6.1,7.0


      Do you want to use clang as CUDA compiler? [y/N]: n
      nvcc will be used as CUDA compiler.

      Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:


      Do you wish to build TensorFlow with MPI support? [y/N]: n
      No MPI support will be enabled for TensorFlow.

      Please specify optimization flags to use during compilation when bazel option "--config=opt" is specified [Default is -march=native]:


      Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
      Not configuring the WORKSPACE for Android builds.

      Preconfigured Bazel build configs. You can use any of the below by adding "--config=<>" to your build command. See tools/bazel.rc for more details.
      --config=mkl         	# Build with MKL support.
      --config=monolithic  	# Config for mostly static monolithic build.
      Configuration finished
    #+END_SRC
  - Build TensorFlow
    - Note that in addition to "opt" and "cuda" I used ~--config=mkl~ that will cause the build to link in the Intel MKL-ML libs. Those libs are now included in the TensorFlow source tree. (Thank you Intel for making those important libraries available to the public.)
    - Also note, I had to add ~--action_env PATH="$PATH"~ because bazel sometimes forgets it's environment!
    - It will take some time to build since TensorFlow is a big package! I was greeted with the wonderful message,
    #+BEGIN_SRC BASH
      # 20GB of RAM and 3 Cores (should be changed based on the system)
      bazel build --local_resources 20000,1.0,2.0 --config=opt --config=mkl --config=cuda --action_env PATH="$PATH"  //tensorflow/tools/pip_package:build_pip_package
    #+END_SRC
  - Create the pip package
    #+BEGIN_SRC BASH
      bazel-bin/tensorflow/tools/pip_package/build_pip_package /root/TF_build/tensorflow_pkg
    #+END_SRC
  - Install the pip package
    #+BEGIN_SRC BASH
      conda create -n tftest

      source activate tftest

      pip install tensorflow_pkg/tensorflow-1.8.0-cp36-cp36m-linux_x86_64.whl
    #+END_SRC
    - To see the linked libraries
      #+BEGIN_SRC BASH
        ldd ~/anaconda3/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.so
        linux-vdso.so.1 =>  (0x00007ffda0d82000)
	    libcublas.so.9.1 => /usr/local/cuda-9.1/targets/x86_64-linux/lib/libcublas.so.9.1 (0x00007fcd69e64000)
	    libcuda.so.1 => /usr/local/nvidia/lib64/libcuda.so.1 (0x00007fcd68ee4000)
	    libcudnn.so.7 => /usr/lib/x86_64-linux-gnu/libcudnn.so.7 (0x00007fcd546bb000)
	    libcufft.so.9.1 => /usr/local/cuda-9.1/targets/x86_64-linux/lib/libcufft.so.9.1 (0x00007fcd4d1ce000)
	    libcurand.so.9.1 => /usr/local/cuda-9.1/targets/x86_64-linux/lib/libcurand.so.9.1 (0x00007fcd4924b000)
	    libcudart.so.9.1 => /usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1 (0x00007fcd48fdd000)
	    libiomp5.so => /root/anaconda3/lib/python3.6/site-packages/tensorflow/../_solib_local/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib/libiomp5.so (0x00007fcd48c39000)
	    libmklml_intel.so => /root/anaconda3/lib/python3.6/site-packages/tensorflow/../_solib_local/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib/libmklml_intel.so (0x00007fcd3fdf6000)
	    libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fcd3fbf2000)
	    libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fcd3f8e9000)
	    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fcd3f6cc000)
	    libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fcd3f34a000)
	    libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fcd3f134000)
	    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fcd3ed6a000)
	    /lib64/ld-linux-x86-64.so.2 (0x00007fcd6ea81000)
	    librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fcd3eb62000)
	    libnvidia-fatbinaryloader.so.396.26 => /usr/local/nvidia/lib64/libnvidia-fatbinaryloader.so.396.26 (0x00007fcd3e916000)
      #+END_SRC

* Prepare the EC2 instance for NVIDIA GPU
  - Steps:
    #+BEGIN_SRC BASH
      # Install official NVIDIA driver package
      sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub
      sudo sh -c 'echo "deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 /" > /etc/apt/sources.list.d/cuda.list'
      sudo apt-get update && sudo apt-get install -y --no-install-recommends linux-headers-generic dkms cuda-drivers

      # Install nvidia-docker and nvidia-docker-plugin
      wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb
      sudo dpkg -i /tmp/nvidia-docker*.deb && rm /tmp/nvidia-docker*.deb
      sudo reboot
    #+END_SRC
  - Refernces 
    - [[https://github.com/NVIDIA/nvidia-docker/wiki/Deploy-on-Amazon-EC2][Deploy on Amazon EC2]]
    - [[https://devblogs.nvidia.com/gpu-containers-runtime/][Enabling GPUs in the Container Runtime Ecosystem]]
    - [[https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-16-04][How To Install and Use Docker on Ubuntu 16.04]]
    - [[https://github.com/NVIDIA/nvidia-container-runtime][nvidia-container-runtime]]
    - [[https://github.com/NVIDIA/libnvidia-container][libnvidia-container]]
